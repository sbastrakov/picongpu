/* Copyright 2013-2019 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "picongpu/simulation_defines.hpp"
#include <pmacc/mappings/threads/ForEachIdx.hpp>
#include <pmacc/mappings/threads/IdxConfig.hpp>
#include <pmacc/mappings/threads/ThreadCollective.hpp>
#include <pmacc/memory/boxes/CachedBox.hpp>


namespace picongpu
{
namespace fields
{
namespace maxwellSolver
{
namespace yeePML
{
    using namespace pmacc;

    /// Parameters and getSigma() should be moved to detail afterwards
    struct Parameters {
        float3_X sigmaMax;
        int gradingOrder;
    };

    DINLINE float3_X getSigma(DataSpace< simDim > cellIdx, Parameters parameters,
        DataSpace< simDim > numPMLCells,
        DataSpace< simDim >localDomainCells)
    {
        parameters.sigmaMax;
    }

    /** compute electric field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateE
    {
        /** update electric field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_SplitEBox PML split electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_BBox)`
         * @param fieldE electric field iterator
         * @param fieldB magnetic field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_EBox,
            typename T_SplitEBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_EBox fieldE,
            T_SplitEBox splitFieldE,
            T_BBox const fieldB, // here only full B is required
            T_Mapping mapper
        ) const
        {
            /// TODO
        }
    };

    /** compute magnetic field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateBHalf
    {
        /** update magnetic field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_SplitBBox PML split magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_EBox)`
         * @param fieldB magnetic field iterator
         * @param fieldE electric field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_PMLBox,
            typename T_EBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_PMLBox splitFields,
            T_BBox fieldB,
            T_EBox const fieldE,
            T_Mapping mapper,
            Parameters parameters
        ) const
        {
            using namespace mappings::threads;

            constexpr uint32_t cellsPerSuperCell = pmacc::math::CT::volume< SuperCellSize >::type::value;
            constexpr uint32_t numWorkers = T_workers;

            uint32_t const workerIdx = threadIdx.x;

            auto const numGuardSuperCells = mapper.getGuardingSuperCells();
            DataSpace< simDim > guardCells(numGuardSuperCells * SuperCellSize::toRT());
            DataSpace< simDim > const localDomainCells = mapper.getGridSuperCells() * SuperCellSize::toRT();
            /// for now assume all guard cells are PML
            auto numPMLCells = guardCells;

            auto cachedE = CachedBox::create<
                0u,
                typename T_EBox::ValueType
            >(
                acc,
                T_BlockDescription( )
            );

            nvidia::functors::Assign assign;
            DataSpace< simDim > const block( mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) ) );
            DataSpace< simDim > const blockCell = block * MappingDesc::SuperCellSize::toRT( );

            auto fieldEBlock = fieldE.shift( blockCell );

            ThreadCollective<
                T_BlockDescription,
                numWorkers
            > collective( workerIdx );

            collective(
                acc,
                assign,
                cachedE,
                fieldEBlock
            );

            __syncthreads();

            constexpr float_X dt = DELTA_T;
            constexpr float_X halfDt = 0.5_X * dt;

            ForEachIdx<
                IdxConfig<
                    cellsPerSuperCell,
                    numWorkers
                >
            >{ workerIdx }(
                [&](
                    uint32_t const linearIdx,
                    uint32_t const
                )
                {
                    /* cell index within the superCell */
                    DataSpace< simDim > const localCellIdx = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                    auto cellIdx = blockCell + localCellIdx;
                    std::cout << "cellIdx, cellIdx = " << cellIdx << " " << localCellIdx << "\n";

                    /// this is to avoid near-boundary values as E spatial derivatives are not available there,
                    /// probably there is a better way
                    if ((cellIdx.x() == localDomainCells.x() - 1) ||
                        (cellIdx.y() == localDomainCells.y() - 1) ||
                        (cellIdx.z() == localDomainCells.z() - 1))
                        return;

                    using Difference = typename T_Curl::Difference;
                    const typename Difference::template GetDifference< 0 > Dx;
                    const typename Difference::template GetDifference< 1 > Dy;
                    const typename Difference::template GetDifference< 2 > Dz;
                    // we might want to precompute that later
                    const float3_X sigma = getSigma( cellIdx, parameters, numPMLCells, localDomainCells );
                    const float3_X damping( math::exp(-sigma[0] * halfDt), math::exp(-sigma[1] * halfDt), math::exp(-sigma[2] * halfDt) );
                    float3_X diff( halfDt, halfDt, halfDt );
                    for (int d = 0; d < 3; d++)
                        if (sigma[d])
                            diff[d] = ( 1.0_X - damping[d] ) / sigma[d];
                    auto localE = cachedE.shift(localCellIdx);
                    splitFields(cellIdx).byx = damping.x() * splitFields(cellIdx).byx + diff.x() * (localE).z();
                    splitFields(cellIdx).bzx = damping.x() * splitFields(cellIdx).bzx - diff.x() * Dx(localE).y();
                    splitFields(cellIdx).bxy = damping.y() * splitFields(cellIdx).bxy - diff.y() * Dy(localE).z();
                    splitFields(cellIdx).bzy = damping.y() * splitFields(cellIdx).bzy + diff.y() * Dy(localE).x();
                    splitFields(cellIdx).bxz = damping.z() * splitFields(cellIdx).bxz + diff.z() * Dz(localE).y();
                    splitFields(cellIdx).byz = damping.z() * splitFields(cellIdx).byz - diff.z() * Dz(localE).x();
                    fieldB(cellIdx).x() = splitFields(cellIdx).bxy + splitFields(cellIdx).bxz;
                    fieldB(cellIdx).y() = splitFields(cellIdx).byx + splitFields(cellIdx).byz;
                    fieldB(cellIdx).z() = splitFields(cellIdx).bzx + splitFields(cellIdx).bzy;
                }
            );
        }
    };

} // namespace yeePML
} // namespace maxwellSolver
} // namespace fields
} // namespace picongpu
