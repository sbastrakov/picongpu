/* Copyright 2013-2019 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Sergei Bastrakov
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "picongpu/simulation_defines.hpp"
#include "picongpu/fields/MaxwellSolver/YeePML/Parameters.hpp"
#include <pmacc/algorithms/math/floatMath/floatingPoint.tpp>
#include <pmacc/mappings/threads/ForEachIdx.hpp>
#include <pmacc/mappings/threads/IdxConfig.hpp>
#include <pmacc/mappings/threads/ThreadCollective.hpp>
#include <pmacc/memory/boxes/CachedBox.hpp>


namespace picongpu
{
namespace fields
{
namespace maxwellSolver
{
namespace yeePML
{

    using namespace pmacc;
    namespace detail
    {

        struct LocalParameters : public Parameters
        {
            floatD_X negativeBorderSize;
            floatD_X positiveBorderSize;

            LocalParameters( Parameters const parameters,
                Thickness const localThickness
            ):
                Parameters( parameters )
            {
                /* Convert size type here to avoid doing that in kernels,
                * reasons for storing size as float_X as are given at
                * KernelParameters definition.
                */
                for( auto axis = 0; axis < simDim; axis++ )
                {
                    negativeBorderSize[ axis ] = static_cast< float_X >(
                        localThickness.negativeBorder[ axis ]
                    );
                    positiveBorderSize[ axis ] = static_cast< float_X >(
                        localThickness.positiveBorder[ axis ]
                    );
                }
            }
        };

        // Get relative depth of a given index in PML, result between 0 and 1
        // index is float to account for halves of cells
        DINLINE float_X getRelativeDepth(
            float_X index,
            float_X numPMLCellsNegative,
            float_X numPMLCellsPositive,
            uint32_t numLocalDomainCells
        )
        {
            float_X result = 0.0;
            if( index < numPMLCellsNegative )
                result = ( numPMLCellsNegative - index ) / numPMLCellsNegative;
            if( index > numLocalDomainCells - numPMLCellsPositive )
                result = ( index - numLocalDomainCells + numPMLCellsPositive ) / numPMLCellsPositive;
            return result;
        }

        DINLINE floatD_X getSigma(
            floatD_X cellIdx,
            LocalParameters const parameters,
            DataSpace< simDim > const numLocalDomainCells
        )
        {
            floatD_X sigma;
            for( uint32_t dim = 0; dim < simDim; dim++ )
            {
                float_X const relativeDepth = getRelativeDepth(
                    cellIdx[ dim ],
                    parameters.negativeBorderSize[ dim ],
                    parameters.positiveBorderSize[ dim ],
                    numLocalDomainCells[ dim ]
                );
                float_X const gradingCoefficient = math::pow( relativeDepth, float_X( parameters.sigmaKappaGradingOrder ) );
                sigma[ dim ] = parameters.normalizedSigmaMax[ dim ] * gradingCoefficient;
            }
            return sigma;
        }

    } // namespace detail

    /** compute electric field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateE
    {
        /** update electric field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_SplitEBox PML split electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_BBox)`
         * @param fieldE electric field iterator
         * @param fieldB magnetic field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_PMLEBox,
            typename T_EBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_PMLEBox splitE,
            T_EBox fieldE,
            T_BBox const fieldB,
            T_Mapping mapper,
            detail::LocalParameters parameters
        ) const
        {
            // All numbers, offsets and indexes in this kernel include guard
            auto const numGuardSuperCells = mapper.getGuardingSuperCells();
            DataSpace< simDim > numGuardCells( numGuardSuperCells * SuperCellSize::toRT() );
            DataSpace< simDim > const numLocalDomainCells = mapper.getGridSuperCells() * SuperCellSize::toRT();

            // local index (inside the local domain), including guards
            DataSpace< simDim > const superCellIdx( mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) ) );
            // starting cell index in the current supercell
            DataSpace< simDim > const startCellIdx = superCellIdx * MappingDesc::SuperCellSize::toRT();

            // Cache E values for the block
            using namespace mappings::threads;
            constexpr uint32_t numWorkers = T_workers;
            uint32_t const workerIdx = threadIdx.x;
            nvidia::functors::Assign assign;
            auto fieldBBlock = fieldB.shift( startCellIdx );
            ThreadCollective<
                T_BlockDescription,
                numWorkers
            > collectiveCacheB( workerIdx );
            auto cachedB = CachedBox::create<
                0u,
                typename T_BBox::ValueType
            >(
                acc,
                T_BlockDescription()
                );
            collectiveCacheB(
                acc,
                assign,
                cachedB,
                fieldBBlock
            );
            __syncthreads();

            // Parallel processing of cells
            constexpr uint32_t numCellsPerSuperCell = pmacc::math::CT::volume< SuperCellSize >::type::value;
            ForEachIdx<
                IdxConfig<
                numCellsPerSuperCell,
                numWorkers
                >
            >{ workerIdx }(
                [&](
                    uint32_t const linearIdx,
                    uint32_t const
                    )
            {
                // cellIdx is cell index in local domain
                DataSpace< simDim > const cellIdxInSuperCell = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                auto cellIdx = startCellIdx + cellIdxInSuperCell;

                // check that the stencil can be computed in our index space
                // to be generic we check both upper and lower sides, even though a
                // stencil is realistically one-sided
                using LowerMargin = typename traits::GetMargin< T_Curl >::LowerMargin;
                using UpperMargin = typename traits::GetMargin< T_Curl >::UpperMargin;
                auto const lowerMargin = LowerMargin::toRT();
                auto const upperMargin = UpperMargin::toRT();
                for ( int dim = 0; dim < simDim; dim++ )
                {
                    bool fitsLower = ( cellIdx[ dim ] >= lowerMargin[ dim ] );
                    bool fitsUpper = ( cellIdx[ dim ] + upperMargin[ dim ] < numLocalDomainCells[ dim ] );
                    if ( !fitsLower || !fitsUpper )
                    {
                        return;
                    }
                }

                constexpr float_X c2 = SPEED_OF_LIGHT * SPEED_OF_LIGHT;
                constexpr float_X dt = DELTA_T;

                // Shift indexes so that 0 is start of the local domain without guard
                const float3_X sigmaIdx(
                    cellIdx.x() - numGuardCells.x(),
                    cellIdx.y() - numGuardCells.y(),
                    cellIdx.z() - numGuardCells.z()
                );
                const float3_X sigma = detail::getSigma( sigmaIdx, parameters, numLocalDomainCells );
                const float3_X damping( 
                    math::exp( -sigma[0] * dt ),
                    math::exp( -sigma[1] * dt ),
                    math::exp( -sigma[2] * dt )
                );
                float3_X diff( dt, dt, dt );
                for ( uint32_t dim = 0; dim < 3; dim++ )
                    if ( sigma[ dim ] )
                        diff[ dim ] = ( 1.0_X - damping[ dim ] ) / sigma[ dim ];
                ///std::cout << "E: cellIdx = " << cellIdx << ", damping = " << damping << ", diff = " << diff << "\n";

                bool inPML = ( sigma.x( ) + sigma.y( ) + sigma.z( ) > 0.0 );
                if( inPML )
                {
                    // Update split fields
                    using Difference = typename T_Curl::Difference;
                    const typename Difference::template GetDifference< 0 > Dx;
                    const typename Difference::template GetDifference< 1 > Dy;
                    const typename Difference::template GetDifference< 2 > Dz;
                    auto const localB = cachedB.shift( cellIdxInSuperCell );
                    splitE( cellIdx ).yx = damping.x() * splitE( cellIdx ).yx - diff.x() * c2 * Dx( localB ).z();
                    splitE( cellIdx ).zx = damping.x() * splitE( cellIdx ).zx + diff.x() * c2 * Dx( localB ).y();
                    splitE( cellIdx ).xy = damping.y() * splitE( cellIdx ).xy + diff.y() * c2 * Dy( localB ).z();
                    splitE( cellIdx ).zy = damping.y() * splitE( cellIdx ).zy - diff.y() * c2 * Dy( localB ).x();
                    splitE( cellIdx ).xz = damping.z() * splitE( cellIdx ).xz - diff.z() * c2 * Dz( localB ).y();
                    splitE( cellIdx ).yz = damping.z() * splitE( cellIdx ).yz + diff.z() * c2 * Dz( localB ).x();

                    // Full fields are sums of split fields
                    fieldE( cellIdx ).x() = splitE( cellIdx ).xy + splitE( cellIdx ).xz;
                    fieldE( cellIdx ).y() = splitE( cellIdx ).yx + splitE( cellIdx ).yz;
                    fieldE( cellIdx ).z() = splitE( cellIdx ).zx + splitE( cellIdx ).zy;
                }
                else
                    fieldE( cellIdx ) += curl( cachedB.shift( cellIdxInSuperCell ) ) * c2 * dt;
            }
            );
        }
    };

    /** Update magnetic field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateBHalf
    {
        /** update magnetic field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_SplitBBox PML split magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_EBox)`
         * @param fieldB magnetic field iterator
         * @param fieldE electric field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_PMLBBox,
            typename T_EBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_PMLBBox splitB,
            T_BBox fieldB,
            T_EBox const fieldE,
            T_Mapping mapper,
            detail::LocalParameters parameters
        ) const
        {
            // All numbers, offsets and indexes in this kernel include guard
            auto const numGuardSuperCells = mapper.getGuardingSuperCells();
            DataSpace< simDim > numGuardCells( numGuardSuperCells * SuperCellSize::toRT() );
            DataSpace< simDim > const numLocalDomainCells = mapper.getGridSuperCells() * SuperCellSize::toRT();

            // local index (inside the local domain), including guards
            DataSpace< simDim > const superCellIdx( mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) ) );
            // starting cell index in the current supercell
            DataSpace< simDim > const startCellIdx = superCellIdx * MappingDesc::SuperCellSize::toRT();

            // Cache E values for the block
            using namespace mappings::threads;
            constexpr uint32_t numWorkers = T_workers;
            uint32_t const workerIdx = threadIdx.x;
            nvidia::functors::Assign assign;
            auto fieldEBlock = fieldE.shift( startCellIdx );
            ThreadCollective<
                T_BlockDescription,
                numWorkers
            > collectiveCacheE( workerIdx );
            auto cachedE = CachedBox::create<
                0u,
                typename T_EBox::ValueType
            >(
                acc,
                T_BlockDescription()
            );
            collectiveCacheE(
                acc,
                assign,
                cachedE,
                fieldEBlock
            );
            __syncthreads();

            // Parallel processing of cells
            constexpr uint32_t numCellsPerSuperCell = pmacc::math::CT::volume< SuperCellSize >::type::value;
            ForEachIdx<
                IdxConfig<
                    numCellsPerSuperCell,
                    numWorkers
                >
            >{ workerIdx }(
                [&](
                    uint32_t const linearIdx,
                    uint32_t const
                )
                {
                    // cellIdx is cell index in local domain
                    DataSpace< simDim > const cellIdxInSuperCell = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                    auto cellIdx = startCellIdx + cellIdxInSuperCell;

                    constexpr float_X halfDt = 0.5_X * DELTA_T;
                    // Shift indexes so that 0 is start of the local domain without guard
                    // With the current grid for B need to shift by a half cell
                    const float3_X sigmaIdx(
                        cellIdx.x() + 0.5_X - numGuardCells.x(),
                        cellIdx.y() + 0.5_X - numGuardCells.y(),
                        cellIdx.z() + 0.5_X - numGuardCells.z()
                    );
                    const float3_X sigma = detail::getSigma( sigmaIdx, parameters, numLocalDomainCells );
                    const float3_X damping(
                        math::exp( -sigma[0] * halfDt ),
                        math::exp( -sigma[1] * halfDt ),
                        math::exp( -sigma[2] * halfDt )
                    );
                    float3_X diff( halfDt, halfDt, halfDt );
                    for ( uint32_t dim = 0; dim < 3; dim++ )
                        if ( sigma[ dim ] )
                            diff[ dim ] = ( 1.0_X - damping[ dim ] ) / sigma[ dim ];
                    ///std::cout << "cellIdx = " << cellIdx << ", damping = " << damping << ", diff = " << diff << "\n";

                    bool inPML = ( sigma.x( ) + sigma.y( ) + sigma.z( ) > 0.0 );
                    if( inPML )
                    {
                        // Update split fields
                        using Difference = typename T_Curl::Difference;
                        const typename Difference::template GetDifference< 0 > Dx;
                        const typename Difference::template GetDifference< 1 > Dy;
                        const typename Difference::template GetDifference< 2 > Dz;
                        auto const localE = cachedE.shift( cellIdxInSuperCell );
                        splitB( cellIdx ).yx = damping.x() * splitB( cellIdx ).yx + diff.x() * Dx( localE ).z();
                        splitB( cellIdx ).zx = damping.x() * splitB( cellIdx ).zx - diff.x() * Dx( localE ).y();
                        splitB( cellIdx ).xy = damping.y() * splitB( cellIdx ).xy - diff.y() * Dy( localE ).z();
                        splitB( cellIdx ).zy = damping.y() * splitB( cellIdx ).zy + diff.y() * Dy( localE ).x();
                        splitB( cellIdx ).xz = damping.z() * splitB( cellIdx ).xz + diff.z() * Dz( localE ).y();
                        splitB( cellIdx ).yz = damping.z() * splitB( cellIdx ).yz - diff.z() * Dz( localE ).x();

                        // Full fields are sums of split fields
                        fieldB( cellIdx ).x() = splitB( cellIdx ).xy + splitB( cellIdx ).xz;
                        fieldB( cellIdx ).y() = splitB( cellIdx ).yx + splitB( cellIdx ).yz;
                        fieldB( cellIdx ).z() = splitB( cellIdx ).zx + splitB( cellIdx ).zy;
                    }
                    else
                    {
                        fieldB( cellIdx ) -= curl( cachedE.shift( cellIdxInSuperCell ) ) * halfDt;
                    }
                }
            );
        }
    };

} // namespace yeePML
} // namespace maxwellSolver
} // namespace fields
} // namespace picongpu
