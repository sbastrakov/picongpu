/* Copyright 2013-2019 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Sergei Bastrakov
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "picongpu/simulation_defines.hpp"
#include <pmacc/algorithms/math/floatMath/floatingPoint.tpp>
#include <pmacc/mappings/threads/ForEachIdx.hpp>
#include <pmacc/mappings/threads/IdxConfig.hpp>
#include <pmacc/mappings/threads/ThreadCollective.hpp>
#include <pmacc/memory/boxes/CachedBox.hpp>


namespace picongpu
{
namespace fields
{
namespace maxwellSolver
{
namespace yeePML
{
    using namespace pmacc;

    namespace detail
    {

        struct Parameters
        {
            // Sizes are float to minimize conversions between
            // integer and floating-point types
            floatD_X negativeBorderSize;
            floatD_X positiveBorderSize;
            floatD_X normalizedSigmaMax;
            uint32_t gradingOrder;
        };

        // Get relative depth of a given index in PML, result between 0 and 1
        // index is float to account for halves of cells
        DINLINE float_X getRelativeDepth(
            float_X index,
            float_X numPMLCellsNegative,
            float_X numPMLCellsPositive,
            uint32_t numLocalDomainCells
        )
        {
            float_X result = 0.0;
            if( index < numPMLCellsNegative )
                result = ( numPMLCellsNegative - index ) / numPMLCellsNegative;
            if( index > numLocalDomainCells - numPMLCellsPositive )
                result = ( index - numLocalDomainCells + numPMLCellsPositive ) / numPMLCellsPositive;
            return result;
        }

        DINLINE floatD_X getSigma(
            floatD_X cellIdx,
            Parameters const parameters,
            DataSpace< simDim > const numLocalDomainCells
        )
        {
            floatD_X sigma;
            for( uint32_t dim = 0; dim < simDim; dim++ )
            {
                float_X const relativeDepth = getRelativeDepth(
                    cellIdx[ dim ],
                    parameters.negativeBorderSize[ dim ],
                    parameters.positiveBorderSize[ dim ],
                    numLocalDomainCells[ dim ]
                );
                float_X const gradingCoefficient = math::pow( relativeDepth, float_X( parameters.gradingOrder ) );
                sigma[ dim ] = parameters.normalizedSigmaMax[ dim ] * gradingCoefficient;
            }
            return sigma;
        }

    } // namespace detail

    /** compute electric field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateE
    {
        /** update electric field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_SplitEBox PML split electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_BBox)`
         * @param fieldE electric field iterator
         * @param fieldB magnetic field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_EBox,
            typename T_PMLBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_PMLBox splitFields,
            T_EBox fieldE,
            T_BBox const fieldB,
            T_Mapping mapper,
            detail::Parameters parameters
        ) const
        {
            // All numbers, offsets and indexes in this kernel include guard
            auto const numGuardSuperCells = mapper.getGuardingSuperCells();
            DataSpace< simDim > numGuardCells( numGuardSuperCells * SuperCellSize::toRT() );
            DataSpace< simDim > const numLocalDomainCells = mapper.getGridSuperCells() * SuperCellSize::toRT();

            // local index (inside the local domain), including guards
            DataSpace< simDim > const superCellIdx( mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) ) );
            // starting cell index in the current supercell
            DataSpace< simDim > const startCellIdx = superCellIdx * MappingDesc::SuperCellSize::toRT();

            // Cache E values for the block
            using namespace mappings::threads;
            constexpr uint32_t numWorkers = T_workers;
            uint32_t const workerIdx = threadIdx.x;
            nvidia::functors::Assign assign;
            auto fieldBBlock = fieldB.shift( startCellIdx );
            ThreadCollective<
                T_BlockDescription,
                numWorkers
            > collectiveCacheB( workerIdx );
            auto cachedB = CachedBox::create<
                0u,
                typename T_BBox::ValueType
            >(
                acc,
                T_BlockDescription()
                );
            collectiveCacheB(
                acc,
                assign,
                cachedB,
                fieldBBlock
            );
            __syncthreads();

            // Parallel processing of cells
            constexpr uint32_t numCellsPerSuperCell = pmacc::math::CT::volume< SuperCellSize >::type::value;
            ForEachIdx<
                IdxConfig<
                numCellsPerSuperCell,
                numWorkers
                >
            >{ workerIdx }(
                [&](
                    uint32_t const linearIdx,
                    uint32_t const
                    )
            {
                // cellIdx is cell index in local domain
                DataSpace< simDim > const cellIdxInSuperCell = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                auto cellIdx = startCellIdx + cellIdxInSuperCell;

                // check that the stencil can be computed in our index space
                // to be generic we check both upper and lower sides, even though a
                // stencil is realistically one-sided
                using LowerMargin = typename traits::GetMargin< T_Curl >::LowerMargin;
                using UpperMargin = typename traits::GetMargin< T_Curl >::UpperMargin;
                auto const lowerMargin = LowerMargin::toRT();
                auto const upperMargin = UpperMargin::toRT();
                for ( int dim = 0; dim < simDim; dim++ )
                {
                    bool fitsLower = ( cellIdx[ dim ] >= lowerMargin[ dim ] );
                    bool fitsUpper = ( cellIdx[ dim ] + upperMargin[ dim ] < numLocalDomainCells[ dim ] );
                    if ( !fitsLower || !fitsUpper )
                    {
                        return;
                    }
                }

                constexpr float_X c2 = SPEED_OF_LIGHT * SPEED_OF_LIGHT;
                constexpr float_X dt = DELTA_T;

                // Shift indexes so that 0 is start of the local domain without guard
                const float3_X sigmaIdx(
                    cellIdx.x() - numGuardCells.x(),
                    cellIdx.y() - numGuardCells.y(),
                    cellIdx.z() - numGuardCells.z()
                );
                const float3_X sigma = detail::getSigma( sigmaIdx, parameters, numLocalDomainCells );
                const float3_X damping( 
                    math::exp( -sigma[0] * dt ),
                    math::exp( -sigma[1] * dt ),
                    math::exp( -sigma[2] * dt )
                );
                float3_X diff( dt, dt, dt );
                for ( uint32_t dim = 0; dim < 3; dim++ )
                    if ( sigma[ dim ] )
                        diff[ dim ] = ( 1.0_X - damping[ dim ] ) / sigma[ dim ];
                ///std::cout << "E: cellIdx = " << cellIdx << ", damping = " << damping << ", diff = " << diff << "\n";

                bool inPML = false;
                if( inPML )
                {
                    // Update split fields
                    using Difference = typename T_Curl::Difference;
                    const typename Difference::template GetDifference< 0 > Dx;
                    const typename Difference::template GetDifference< 1 > Dy;
                    const typename Difference::template GetDifference< 2 > Dz;
                    auto & const localB = cachedB.shift( cellIdxInSuperCell );
                    splitFields( cellIdx ).eyx = damping.x() * splitFields( cellIdx ).eyx - diff.x() * c2 * Dx( localB ).z();
                    splitFields( cellIdx ).ezx = damping.x() * splitFields( cellIdx ).ezx + diff.x() * c2 * Dx( localB ).y();
                    splitFields( cellIdx ).exy = damping.y() * splitFields( cellIdx ).exy + diff.y() * c2 * Dy( localB ).z();
                    splitFields( cellIdx ).ezy = damping.y() * splitFields( cellIdx ).ezy - diff.y() * c2 * Dy( localB ).x();
                    splitFields( cellIdx ).exz = damping.z() * splitFields( cellIdx ).exz - diff.z() * c2 * Dz( localB ).y();
                    splitFields( cellIdx ).eyz = damping.z() * splitFields( cellIdx ).eyz + diff.z() * c2 * Dz( localB ).x();

                    // Full fields are sums of split fields
                    fieldE( cellIdx ).x() = splitFields( cellIdx ).exy + splitFields( cellIdx ).exz;
                    fieldE( cellIdx ).y() = splitFields( cellIdx ).eyx + splitFields( cellIdx ).eyz;
                    fieldE( cellIdx ).z() = splitFields( cellIdx ).ezx + splitFields( cellIdx ).ezy;
                }
                else
                    fieldE( cellIdx ) += curl( cachedB.shift( cellIdxInSuperCell ) ) * c2 * dt;
            }
            );
        }
    };

    /** Update magnetic field
     *
     * @tparam T_numWorkers number of workers
     * @tparam T_BlockDescription field (electric and magnetic) domain description
     */
    template<
        uint32_t T_workers,
        typename T_BlockDescription
    >
    struct KernelUpdateBHalf
    {
        /** update magnetic field
         *
         * @tparam T_Curl curl functor type
         * @tparam T_EBox pmacc::DataBox, electric field box type
         * @tparam T_BBox pmacc::DataBox, magnetic field box type
         * @tparam T_SplitBBox PML split magnetic field box type
         * @tparam T_Mapping mapper functor type
         * @tparam T_Acc alpaka accelerator type
         *
         * @param acc alpaka accelerator
         * @param curl functor to calculate the electric field, interface must be
         *             `operator()(T_EBox)`
         * @param fieldB magnetic field iterator
         * @param fieldE electric field iterator
         * @param mapper functor to map a block to a supercell
         */
        template<
            typename T_Curl,
            typename T_PMLBox,
            typename T_EBox,
            typename T_BBox,
            typename T_Mapping,
            typename T_Acc
        >
        DINLINE void operator()(
            T_Acc const & acc,
            T_Curl const curl,
            T_PMLBox splitFields,
            T_BBox fieldB,
            T_EBox const fieldE,
            T_Mapping mapper,
            detail::Parameters parameters
        ) const
        {
            // All numbers, offsets and indexes in this kernel include guard
            auto const numGuardSuperCells = mapper.getGuardingSuperCells();
            DataSpace< simDim > numGuardCells( numGuardSuperCells * SuperCellSize::toRT() );
            DataSpace< simDim > const numLocalDomainCells = mapper.getGridSuperCells() * SuperCellSize::toRT();

            // local index (inside the local domain), including guards
            DataSpace< simDim > const superCellIdx( mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) ) );
            // starting cell index in the current supercell
            DataSpace< simDim > const startCellIdx = superCellIdx * MappingDesc::SuperCellSize::toRT();

            // Cache E values for the block
            using namespace mappings::threads;
            constexpr uint32_t numWorkers = T_workers;
            uint32_t const workerIdx = threadIdx.x;
            nvidia::functors::Assign assign;
            auto fieldEBlock = fieldE.shift( startCellIdx );
            ThreadCollective<
                T_BlockDescription,
                numWorkers
            > collectiveCacheE( workerIdx );
            auto cachedE = CachedBox::create<
                0u,
                typename T_EBox::ValueType
            >(
                acc,
                T_BlockDescription()
            );
            collectiveCacheE(
                acc,
                assign,
                cachedE,
                fieldEBlock
            );
            __syncthreads();

            // Parallel processing of cells
            constexpr uint32_t numCellsPerSuperCell = pmacc::math::CT::volume< SuperCellSize >::type::value;
            ForEachIdx<
                IdxConfig<
                    numCellsPerSuperCell,
                    numWorkers
                >
            >{ workerIdx }(
                [&](
                    uint32_t const linearIdx,
                    uint32_t const
                )
                {
                    // cellIdx is cell index in local domain
                    DataSpace< simDim > const cellIdxInSuperCell = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                    auto cellIdx = startCellIdx + cellIdxInSuperCell;

                    constexpr float_X halfDt = 0.5_X * DELTA_T;
                    // Shift indexes so that 0 is start of the local domain without guard
                    // With the current grid for B need to shift by a half cell
                    const float3_X sigmaIdx(
                        cellIdx.x() + 0.5_X - numGuardCells.x(),
                        cellIdx.y() + 0.5_X - numGuardCells.y(),
                        cellIdx.z() + 0.5_X - numGuardCells.z()
                    );
                    const float3_X sigma = detail::getSigma( sigmaIdx, parameters, numLocalDomainCells );
                    const float3_X damping(
                        math::exp( -sigma[0] * halfDt ),
                        math::exp( -sigma[1] * halfDt ),
                        math::exp( -sigma[2] * halfDt )
                    );
                    float3_X diff( halfDt, halfDt, halfDt );
                    for ( uint32_t dim = 0; dim < 3; dim++ )
                        if ( sigma[ dim ] )
                            diff[ dim ] = ( 1.0_X - damping[ dim ] ) / sigma[ dim ];
                    ///std::cout << "cellIdx = " << cellIdx << ", damping = " << damping << ", diff = " << diff << "\n";

                    bool inPML = false;
                    if( inPML )
                    {
                        // Update split fields
                        using Difference = typename T_Curl::Difference;
                        const typename Difference::template GetDifference< 0 > Dx;
                        const typename Difference::template GetDifference< 1 > Dy;
                        const typename Difference::template GetDifference< 2 > Dz;
                        auto & const localE = cachedE.shift( cellIdxInSuperCell );
                        splitFields( cellIdx ).byx = damping.x() * splitFields( cellIdx ).byx + diff.x() * Dx( localE ).z();
                        splitFields( cellIdx ).bzx = damping.x() * splitFields( cellIdx ).bzx - diff.x() * Dx( localE ).y();
                        splitFields( cellIdx ).bxy = damping.y() * splitFields( cellIdx ).bxy - diff.y() * Dy( localE ).z();
                        splitFields( cellIdx ).bzy = damping.y() * splitFields( cellIdx ).bzy + diff.y() * Dy( localE ).x();
                        splitFields( cellIdx ).bxz = damping.z() * splitFields( cellIdx ).bxz + diff.z() * Dz( localE ).y();
                        splitFields( cellIdx ).byz = damping.z() * splitFields( cellIdx ).byz - diff.z() * Dz( localE ).x();

                        // Full fields are sums of split fields
                        fieldB( cellIdx ).x() = splitFields( cellIdx ).bxy + splitFields( cellIdx ).bxz;
                        fieldB( cellIdx ).y() = splitFields( cellIdx ).byx + splitFields( cellIdx ).byz;
                        fieldB( cellIdx ).z() = splitFields( cellIdx ).bzx + splitFields( cellIdx ).bzy;
                    }
                    else
                    {
                        fieldB( cellIdx ) -= curl( cachedE.shift( cellIdxInSuperCell ) ) * halfDt;
                    }
                }
            );
        }
    };

} // namespace yeePML
} // namespace maxwellSolver
} // namespace fields
} // namespace picongpu
