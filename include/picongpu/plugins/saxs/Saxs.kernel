/* Copyright 2013-2020 Axel Huebl, Heiko Burau, Rene Widera, Richard Pausch,
 *                     Klaus Steiniger, Felix Schmitt, Benjamin Worpitz,
 *                     Juncheng E, Sergei Bastrakov
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "picongpu/simulation_defines.hpp"

#include <pmacc/dimensions/DataSpaceOperations.hpp>
#include <pmacc/mappings/threads/ForEachIdx.hpp>
#include <pmacc/mappings/threads/IdxConfig.hpp>
#include <pmacc/memory/Array.hpp>
#include <pmacc/memory/shared/Allocate.hpp>
#include <pmacc/nvidia/atomic.hpp>
#include <pmacc/nvidia/functors/Add.hpp>

#include <cstdint>


namespace picongpu
{
namespace plugins
{
namespace saxs
{
namespace detail
{

    /** Parameters of the reciprocal space
     * 
     * They define the regular 3d Cartesian lattice of scattering vectors q
     */
    struct ReciprocalSpace
    {
        //! Lattice start
        float3_X min;

        //! Lattice step
        float3_X step;

        //! Number of points per each direction
        pmacc::DataSpace< 3 > size;

        /** Create a reciprical space
         *
         * @param min lattice start
         * @param step lattice step
         * @param size number of points per each direction
         */
        HDINLINE ReciprocalSpace(
            float3_X & const min = float3_X::create( 0._X ),
            float3_X & const step = float3_X::create( 0._X ),
            pmacc::DataSpace< 3 > & const size = DataSpace< 3 >::create( 1 )
        ):
            min( min ),
            step( step ),
            size( size )
        {
        }

        /** Get scattering vector value by linear index
         *
         * @param linearIdx linear index within the product of size components
         */
        HDINLINE float3_X getValue( uint32_t const linearIdx ) const
        {
            pmacc::DataSpace< 3 > idx;   
            idx[ 2 ] = linearIdx % size.z();
            idx[ 1 ] = ( linearIdx / size.z() ) % size.y();
            idx[ 0 ] = linearIdx / ( size.z() * size.y() );
            return min + step * precisionCast< float_X >( idx );
        }

    };

    //! Internal representation of results
    struct Result
    {
        //! 
        float_X sumfcoskr = 0._X;
        float_X sumfsinkr = 0._X;
        
        //! Number of processed macroparticles
        uint32_t totalNumMacroparticles = 0u;

        //! Combined weighting of processed macroparticles
        float_X totalWeighting = 0._X;

        Result & operator+=( Result const & other )
        {
            sumfcoskr += other.sumfcoskr;
            sumfsinkr += other.sumfsinkr;
            totalWeighting += other.totalWeighting;
            totalNumMacroparticles += other.totalNumMacroparticles;
            return *this;
        }

    };
 
    template<
        uint32_t T_numWorkers,
        typename T_Acc,
        typename ParBox,
        typename T_Mapping,
        typename T_CachedPositionArray,
        typename T_CachedWeightingArray
    >
    DINLINE Result processSupercell(
        T_Acc const &acc,
        DataSpace< simDim > const & idx,
        DataSpace< simDim > const & globalOffset,
        T_Mapping mapper,
        ParBox pb,
        T_CachedPositionArray & cachedR,
        T_CachedWeightingArray & cachedWeighting,
        ReciprocalSpace const & reciprocalSpace
    )
    {
        auto const guardingSuperCells = mapper.getGuardingSuperCells();
        auto const offset = globalOffset +
            ( ( idx - guardingSuperCells ) * SuperCellSize::toRT() );
        Result result;
        auto frame = pb.getLastFrame( idx );
        auto particlesInFrame = pb.getSuperCell( idx ).getSizeLastFrame();
        while( frame.isValid() )
        {
            auto frameResult = processFrame< T_numWorkers >(
                acc,
                frame,
                particlesInFrame,
                cachedR,
                cachedWeighting,
                offset,
                reciprocalSpace
            );
            result += frameResult;
            frame = pb.getPreviousFrame( frame );
            particlesInFrame = pmacc::math::CT::volume< SuperCellSize >::type::value; /// fix computation from particle box
            __syncthreads();
        }
        result.totalNumMacroparticles = pb.getSuperCell( idx ).getNumParticles();
        return result;
    }

    template<
        uint32_t T_numWorkers,
        typename T_Acc,
        typename T_Frame,
        typename T_CachedPositionArray,
        typename T_CachedWeightingArray
    >
        DINLINE Result processFrame(
            T_Acc const & acc,
            T_Frame const & frame,
            uint32_t const numParticlesInFrame,
            T_CachedPositionArray & cachedR,
            T_CachedWeightingArray & cachedWeighting,
            DataSpace< simDim > supercellOffset,
            ReciprocalSpace const & reciprocalSpace
        )
    {
        uint32_t const workerIdx = threadIdx.x;
        uint32_t const globalWorkerIdx = blockIdx.x * blockDim.x + threadIdx.x;
        using namespace pmacc::mappings::threads;

        // Cache particle data of this frame to shared memory
        constexpr uint32_t frameSize =
            pmacc::math::CT::volume< SuperCellSize >::type::value; /// fix computation from particle box

        using ParticleDomCfg = IdxConfig<
            frameSize,
            T_numWorkers
        >;
        ForEachIdx< ParticleDomCfg > forEachParticle{ workerIdx };
        forEachParticle([&](uint32_t const linearIdx, uint32_t const)
        {
            if( linearIdx < numParticlesInFrame )
            {
                auto particle = frame[ linearIdx ];
                auto const cellIdx = particle[ localCellIdx_ ];
                auto const pos = particle[ position_ ];
                // calculate global position of cell
                DataSpace<simDim> const globalPos(
                    supercellOffset +
                    DataSpaceOperations<simDim>::template map<
                    SuperCellSize>(cellIdx));
                // Set z component to zero for 2D
                cachedR[ linearIdx ][ 2 ] = 0.0;
                for( auto dim = 0u; dim < simDim; ++dim )
                    cachedR[ linearIdx ][ dim ] =
                    (static_cast< float_X >(globalPos[dim]) + pos[dim]) * cellSize[dim];
                cachedWeighting[ linearIdx ] = particle[ weighting_ ];

            }
        });

        __syncthreads(); // wait till every thread has loaded its
                         // particle data

        // conversion multiplier from PIC unit length to angstrom
        constexpr float_X meter2angstrom = 1e10_X;
        constexpr float_X lengthToAngstrom = meter2angstrom * UNIT_LENGTH;

        auto const q = reciprocalSpace.getValue( globalWorkerIdx );

        // Particle loop: thread runs through loaded particle data
        Result result;
        result.totalNumMacroparticles = numParticlesInFrame;
        for( auto particleIdx = 0u; particleIdx < numParticlesInFrame; ++particleIdx )
        {
            auto const dotkr = math::dot(
                q,
                cachedR[ particleIdx ]
            ) * lengthToAngstrom;
            float_X sinValue, cosValue;
            math::sincos(
                dotkr,
                sinValue,
                cosValue
            );
            result.sumfcoskr += cosValue * cachedWeighting[ particleIdx ];
            result.sumfsinkr += sinValue * cachedWeighting[ particleIdx ];
            result.totalWeighting += cachedWeighting[ particleIdx ];
        }
        return result;
    }

    /** calculate the scattering of a species
     *
     * @tparam T_numWorkers number of workers
     */
    template< uint32_t T_numWorkers >
    struct KernelSaxs
    {
        /**
         * The SAXS kernel calculates for all particles on the device the
         * scattering intensity for input calculation ranges.
         * The parallelization is as follows:
         *  - The number of threads per block is equal to the number of cells per
         *    super cells which is also equal to the number of particles per frame
         *
         * The procedure starts with calculating unique ids for the threads and
         * initializing the shared memory.
         * Then a loop over all super cells starts.
         * Every thread loads a particle from that super cell and calculates its
         * scattering structure factor.
         * For every Particle
         * exists therefore a unique space within the shared memory.
         * After that, a thread calculates for a specific scattering vector of all
         * particles.
         */
        template <
            typename ParBox,
            typename DBox,
            typename DBox_np,
            typename DBox_nmp,
            typename Mapping,
            typename T_Acc>
        DINLINE
        void
        operator()(
            T_Acc const &acc,
            ParBox pb,
            DBox sumfcoskr,
            DBox sumfsinkr,
            DBox_np np,
            DBox_nmp nmp,
            DataSpace< simDim > globalOffset,
            Mapping mapper,
            ReciprocalSpace const & reciprocalSpace
        ) const
        {
            constexpr uint32_t frameSize =
                pmacc::math::CT::volume< SuperCellSize >::type::value; /// fix computation from particle box

            // vectorial part of the integrand in the Jackson formula
            using PositionArray = memory::Array<
                float3_X,
                frameSize
            >;
            PMACC_SMEM(
                acc,
                cachedR,
                PositionArray
            );

            using WeightingArray = memory::Array<
                float_X,
                frameSize
            >;
            PMACC_SMEM(
                acc,
                cachedWeighting,
                WeightingArray
            );


            /* number of super cells on GPU per dimension (still including guard
             * cells) remove both guards from count [later one sided guard needs to
             * be added again]
             */
            auto const guardingSuperCells = mapper.getGuardingSuperCells();
            auto const superCellsCount =
                mapper.getGridSuperCells() - 2 * guardingSuperCells;

            // get absolute number of relevant super cells
            auto const numSuperCells = superCellsCount.productOfComponents();

            detail::Result result;
            for( auto supercellLinearIdx = 0; supercellLinearIdx < numSuperCells; ++supercellLinearIdx )
            {             
                auto const idxWithGuard = DataSpaceOperations< simDim >::map(
                    superCellsCount,
                    supercellLinearIdx
                ) + guardingSuperCells;

                auto const supercellResult = detail::processSupercell< T_numWorkers >(
                    acc,
                    idxWithGuard,
                    globalOffset,
                    mapper,
                    pb,
                    cachedR,
                    cachedWeighting,
                    reciprocalSpace
                );
               /* Note: summing up for each supercell separately and adding
                * to the total has an added benefit of reducing summation errors
                * compared to a straighforward summation
                */
                result += supercellResult;           
            }

            // write results to global memory
            auto const globalWorkerIdx = blockIdx.x * blockDim.x + threadIdx.x;
            sumfcoskr[ globalWorkerIdx ] = result.sumfcoskr;
            sumfsinkr[ globalWorkerIdx ] = result.sumfsinkr;
            // total weight of particles is written by a single thread per GPU
            if( globalWorkerIdx == 0 )
            {
                np[ 0 ] = result.totalWeighting;
                nmp[ 0 ] = result.totalNumMacroparticles;
            }

        }
    };

} // namespace detail
} // namespace saxs
} // namespae plugins
} // namespace picongpu
